2025-02-20 13:08:48,651 - DEBUG - utils.py - Attempting to read YAML file from './configs/train_config_llama.yaml'
2025-02-20 13:08:48,655 - INFO - utils.py - YAML file './configs/train_config_llama.yaml' loaded successfully.
2025-02-20 13:08:48,655 - DEBUG - utils.py - Configuration loaded: {'dataset_config': {'desc': 'Multilinuagal sentiment classification dataset', 'train': {'dataset_path': 'tyqiangz/multilingual-sentiments', 'dataset_language': 'english', 'return_dict': True, 'padding': 'longest', 'max_length': 512, 'sub_split_size': None}, 'val': {'dataset_path': 'tyqiangz/multilingual-sentiments', 'dataset_language': 'english', 'return_dict': True, 'padding': 'longest', 'max_length': 512, 'sub_split_size': None}}, 'tokenizer_config': {'tokenizer_name': 'llama2', 'tokenizer_path': 'meta-llama/Llama-2-7b-hf'}, 'model_config': {'model_name': 'llama2-base', 'base_model_path': 'meta-llama/Llama-2-7b-hf', 'num_labels': 3, 'force_download': False, 'device_map': 'cuda:0', 'bnb_config': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': False}}, 'lora_config': {'r': 16, 'lora_alpha': 64, 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'SEQ_CLS', 'target_modules': ['q_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'v_proj']}, 'trainer_config': {'save_trained_model': True, 'save_strategy': 'no', 'save_total_limit': 2, 'run_name': 'llama2_english', 'report_to': 'tensorboard', 'output_dir': './TrainingLogs/', 'overwrite_output_dir': True, 'fp16': True, 'do_train': True, 'do_eval': True, 'eval_strategy': 'epoch', 'num_train_epochs': 5, 'per_device_train_batch_size': 5, 'prediction_loss_only': False, 'logging_strategy': 'steps', 'logging_steps': 2, 'save_steps': 25, 'max_grad_norm': 0.3, 'seed': 'random', 'warmup_ratio': 0.05, 'learning_rate': 0.0001, 'lr_scheduler_type': 'cosine', 'optim': 'paged_adamw_32bit'}, 'prediction_config': {'samples': ['this is a good product', 'this is a bad product', 'this is a very stupid product']}}
2025-02-20 13:08:48,655 - INFO - utils.py - Loaded tokenizer configuration: {'tokenizer_name': 'llama2', 'tokenizer_path': 'meta-llama/Llama-2-7b-hf'}
2025-02-20 13:08:48,656 - INFO - model.py - Initializing tokenizer 'llama2' with arguments: {'tokenizer_path': 'meta-llama/Llama-2-7b-hf', 'config': {}}
2025-02-20 13:08:48,859 - DEBUG - model.py - Special tokens added to the tokenizer.
2025-02-20 13:08:48,859 - INFO - utils.py - Loaded dataset configuration: {'return_dict': True, 'padding': 'longest', 'max_length': 512, 'sub_split_size': None}
2025-02-20 13:08:49,978 - INFO - train.py - Loaded Train Dataset: Multilinuagal sentiment classification dataset, Dataset Length: 1839 with sub_split_size None.
2025-02-20 13:08:49,978 - INFO - utils.py - Loaded dataset configuration: {'return_dict': True, 'padding': 'longest', 'max_length': 512, 'sub_split_size': None}
2025-02-20 13:08:50,454 - INFO - train.py - Loaded Val Dataset: Multilinuagal sentiment classification dataset, Dataset Length: 324.
2025-02-20 13:08:50,454 - INFO - utils.py - Loaded model configuration: {'model_name': 'llama2-base', 'base_model_path': 'meta-llama/Llama-2-7b-hf', 'num_labels': 3, 'force_download': False, 'device_map': 'cuda:0', 'bnb_config': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': False}}
2025-02-20 13:08:50,454 - INFO - model.py - Initializing model 'llama2-base' with arguments: {'base_model_path': 'meta-llama/Llama-2-7b-hf', 'config': {'num_labels': 3, 'force_download': False, 'device_map': 'cuda:0', 'bnb_config': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': False}, 'pad_token_id': 2}}
2025-02-20 13:08:53,881 - DEBUG - model.py - Base model loaded successfully.
2025-02-20 13:08:53,887 - INFO - utils.py - Loaded trainer configuration: {'r': 16, 'lora_alpha': 64, 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'SEQ_CLS', 'target_modules': ['q_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'v_proj']}
2025-02-20 13:08:54,373 - INFO - utils.py - Loaded trainer configuration: {'save_trained_model': True, 'save_strategy': 'no', 'save_total_limit': 2, 'run_name': 'llama2_english', 'report_to': 'tensorboard', 'output_dir': './TrainingLogs/', 'overwrite_output_dir': True, 'fp16': True, 'do_train': True, 'do_eval': True, 'eval_strategy': 'epoch', 'num_train_epochs': 5, 'per_device_train_batch_size': 5, 'prediction_loss_only': False, 'logging_strategy': 'steps', 'logging_steps': 2, 'save_steps': 25, 'max_grad_norm': 0.3, 'seed': 3170428613, 'warmup_ratio': 0.05, 'learning_rate': 0.0001, 'lr_scheduler_type': 'cosine', 'optim': 'paged_adamw_32bit'}
2025-02-20 13:08:54,407 - WARNING - other.py - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-02-20 13:08:54,461 - INFO - train.py - Training started.

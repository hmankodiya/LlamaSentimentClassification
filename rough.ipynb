{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_seq_classifier = GPT2ForSequenceClassification.from_pretrained(\n",
    "    \"openai-community/gpt2\"\n",
    ").to(device=\"cuda:0\")\n",
    "gpt_seq_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_tensor = torch.randn((1, 10, 768)).to(device=\"cuda:0\")\n",
    "rand_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.1311, -5.3402]], device='cuda:0'), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out_logits = gpt_seq_classifier(inputs_embeds=rand_tensor)\n",
    "out_logits.logits, out_logits.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Sepatutnya berbuat begitu\\xa0 demi untuk menawarkan sesuatu yang lebih baik kepada rakyat.',\n",
       " 'source': 'malaya',\n",
       " 'language': 'malay',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"all\", split=\"test\")\n",
    "train_dataset[10]\n",
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/DL/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from sentiment_dataset import SentimentDataset\n",
    "from model import load_tokenizer, load_model\n",
    "from utils import read_yaml, get_tokenizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_yaml(\"./configs/train_config.yaml\")\n",
    "\n",
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[19117,   265,   315,  3281,    64, 18157, 11110,   265,  4123, 34272,\n",
       "           1849,  1357,    72,  1418,  2724,  1450,   707,   668,   272,   264,\n",
       "            274,    84, 33419,   331,   648,   443,    65,  4449, 26605,  1134,\n",
       "            885,    79,  4763,   374, 15492,   265,    13]]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dataset = SentimentDataset(\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "next(iter(sentiment_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"Cant wait to watch the debate tomorrow night. If Romney grills Obama as hard as he did Gingrich in FL, Romney will wipe the floor with Obama\" ',\n",
       " 'source': 'sem_eval_2017',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "english_dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", 'english', split='train')\n",
    "english_dataset[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'yang memerlukan pemerhatian dan tindakan serius',\n",
       " 'source': 'malaya',\n",
       " 'language': 'malay',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", 'all', split='train')\n",
    "all_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_dataset.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "japanese      120000\n",
       "chinese       120000\n",
       "indonesian     11000\n",
       "malay           4687\n",
       "hindi           1839\n",
       "german          1839\n",
       "italian         1839\n",
       "english         1839\n",
       "portuguese      1839\n",
       "french          1839\n",
       "spanish         1839\n",
       "arabic          1839\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_dataset:\n",
    "    if i['language']=='malaya':\n",
    "        out = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'okay i\\\\u2019m sorry but TAYLOR SWIFT LOOKS NOTHING LIKE JACKIE O SO STOP COMPARING THE TWO. c\\\\u2019mon America aren\\\\u2019t you sick of her yet? (sorry) ',\n",
       " 'source': 'sem_eval_2017',\n",
       " 'language': 'english',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentiment_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_sentiment_dataset\n\u001b[0;32m----> 2\u001b[0m dataset_temp \u001b[38;5;241m=\u001b[39m load_sentiment_dataset(\u001b[43mdataset_path\u001b[49m, dataset_language, split)\n\u001b[1;32m      3\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m      4\u001b[0m dataset_temp[index]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_path' is not defined"
     ]
    }
   ],
   "source": [
    "from sentiment_dataset import load_sentiment_dataset\n",
    "dataset_temp = load_sentiment_dataset(dataset_path, dataset_language, split)\n",
    "index = 6\n",
    "dataset_temp[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import predict\n",
    "\n",
    "prediction_config, prompt_samples = get_predicbtion_config(config, pop_samples=True)\n",
    "decoded_outputs = predict(\n",
    "    model, tokenizer, dataset_temp[index][\"text\"], **prediction_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################ \n",
    "use_4bit = True # Activate 4-bit precision base model loading\n",
    "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models \n",
    "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
    "use_nested_quant = False # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668dcd2478d44d4988a15b6c6af06cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=bnb_config,\n",
    "    num_labels=3,\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,989,248 || all params: 6,647,345,152 || trainable%: 0.6016\n"
     ]
    }
   ],
   "source": [
    "lora_r = 16\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.1\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, how are you?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs.input_ids.to(device='cuda:0'))\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_ids = model.generate(inputs.input_ids.to(device='cuda:0'), max_length=100)\n",
    "# tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import evaluate\n",
    "\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_split_config,\n",
    "    get_dataset_config,\n",
    "    get_trainer_config,\n",
    "    get_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_lora_config,\n",
    "    _handle_seed,\n",
    "    DEVICE,\n",
    ")\n",
    "from sentiment_dataset import SentimentDataset\n",
    "from model import load_model, load_tokenizer, predict, compute_metrics, load_lora_model\n",
    "\n",
    "METRICS_DICT = dict(accuracy=evaluate.load('accuracy'), auc=evaluate.load('roc_auc', 'multiclass'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'llama2-base',\n",
       " 'base_model_path': 'meta-llama/Llama-2-7b-hf',\n",
       " 'num_labels': 3,\n",
       " 'force_download': False,\n",
       " 'device_map': 'cuda:0',\n",
       " 'bnb_config': {'load_in_4bit': True,\n",
       "  'bnb_4bit_quant_type': 'nf4',\n",
       "  'bnb_4bit_compute_dtype': 'float16',\n",
       "  'bnb_4bit_use_double_quant': False}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = read_yaml(\"./configs/train_config_llama.yaml\")\n",
    "config['model_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama2-base',\n",
       " None,\n",
       " 'meta-llama/Llama-2-7b-hf',\n",
       " {'num_labels': 3,\n",
       "  'force_download': False,\n",
       "  'device_map': 'cuda:0',\n",
       "  'bnb_config': {'load_in_4bit': True,\n",
       "   'bnb_4bit_quant_type': 'nf4',\n",
       "   'bnb_4bit_compute_dtype': 'float16',\n",
       "   'bnb_4bit_use_double_quant': False}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name, model_path, base_model_path, model_config = get_model_config(config)\n",
    "model_name, model_path, base_model_path, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f591b3e837604e83b15f89283840c5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_labels': 3, 'force_download': False, 'device_map': 'cuda:0'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(\n",
    "    model_string=model_name,\n",
    "    model_path=model_path,\n",
    "    base_model_path=base_model_path,\n",
    "    model_config=model_config,\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dataset_path': 'tyqiangz/multilingual-sentiments',\n",
       "  'dataset_language': 'english',\n",
       "  'return_dict': True,\n",
       "  'padding': 'longest',\n",
       "  'max_length': 512,\n",
       "  'sub_split_size': 0.7},\n",
       " {'dataset_path': 'tyqiangz/multilingual-sentiments',\n",
       "  'dataset_language': 'english',\n",
       "  'return_dict': True,\n",
       "  'padding': 'longest',\n",
       "  'max_length': 512,\n",
       "  'sub_split_size': 0.1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_desc, (train_split_config, val_split_config, test_split_config) = get_split_config(config)\n",
    "train_split_config, val_split_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path, train_dataset_language, train_dataset_config = get_dataset_config(\n",
    "    train_split_config\n",
    ")\n",
    "train_dataset = SentimentDataset(\n",
    "    dataset_path=train_dataset_path,\n",
    "    dataset_language='all',\n",
    "    split_type=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    "    **train_dataset_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(train_dataset.dataset.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['language'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_path, val_dataset_language, val_dataset_config = get_dataset_config(\n",
    "    val_split_config\n",
    ")\n",
    "val_dataset = SentimentDataset(\n",
    "    dataset_path=val_dataset_path,\n",
    "    dataset_language=val_dataset_language,\n",
    "    split_type=\"validation\",\n",
    "    tokenizer=tokenizer,\n",
    "    **val_dataset_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = get_lora_config(config)\n",
    "model = load_lora_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,989,248 || all params: 6,647,345,152 || trainable%: 0.6016\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=train_dataset_config.get(\"padding\", True),\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = get_trainer_config(config)\n",
    "trainer_config[\"logging_dir\"] = os.path.join(\n",
    "    trainer_config[\"output_dir\"], \"runs\", trainer_config[\"run_name\"]\n",
    ")\n",
    "save_trained_model = trainer_config.pop(\"save_trained_model\", True)\n",
    "trainer_args = TrainingArguments(**trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1068, -0.5030,  2.0381]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(\"hello world\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outs = model(inputs.input_ids.to(device=\"cuda:0\"))\n",
    "outs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74403768d9134f2eb609d41692e07215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/harsh/anaconda3/envs/DL/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5327, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.01}\n",
      "{'loss': 2.4203, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.02}\n",
      "{'loss': 1.6246, 'grad_norm': 24.94619369506836, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.02}\n",
      "{'loss': 2.3152, 'grad_norm': 86.29071807861328, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.03}\n",
      "{'loss': 1.6538, 'grad_norm': 79.77373504638672, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.04}\n",
      "{'loss': 2.6576, 'grad_norm': 50.0856819152832, 'learning_rate': 2.6923076923076923e-05, 'epoch': 0.05}\n",
      "{'loss': 1.475, 'grad_norm': 26.794532775878906, 'learning_rate': 3.461538461538462e-05, 'epoch': 0.05}\n",
      "{'loss': 2.2248, 'grad_norm': 72.29523468017578, 'learning_rate': 4.230769230769231e-05, 'epoch': 0.06}\n",
      "{'loss': 1.735, 'grad_norm': 28.304706573486328, 'learning_rate': 5e-05, 'epoch': 0.07}\n",
      "{'loss': 1.7358, 'grad_norm': 58.76654815673828, 'learning_rate': 5.769230769230769e-05, 'epoch': 0.08}\n",
      "{'loss': 2.4394, 'grad_norm': 75.84905242919922, 'learning_rate': 6.538461538461539e-05, 'epoch': 0.09}\n",
      "{'loss': 2.2754, 'grad_norm': 51.812747955322266, 'learning_rate': 7.307692307692307e-05, 'epoch': 0.09}\n",
      "{'loss': 2.0186, 'grad_norm': 58.64046859741211, 'learning_rate': 8.076923076923078e-05, 'epoch': 0.1}\n",
      "{'loss': 1.8017, 'grad_norm': 87.84193420410156, 'learning_rate': 8.846153846153847e-05, 'epoch': 0.11}\n",
      "{'loss': 2.0692, 'grad_norm': 73.14095306396484, 'learning_rate': 9.615384615384617e-05, 'epoch': 0.12}\n",
      "{'loss': 1.3663, 'grad_norm': 58.433738708496094, 'learning_rate': 9.99989723479183e-05, 'epoch': 0.12}\n",
      "{'loss': 1.6243, 'grad_norm': 41.12517547607422, 'learning_rate': 9.999075138471951e-05, 'epoch': 0.13}\n",
      "{'loss': 1.7055, 'grad_norm': 44.06288528442383, 'learning_rate': 9.99743108100344e-05, 'epoch': 0.14}\n",
      "{'loss': 1.3654, 'grad_norm': 74.14151000976562, 'learning_rate': 9.994965332706573e-05, 'epoch': 0.15}\n",
      "{'loss': 0.9701, 'grad_norm': 61.184661865234375, 'learning_rate': 9.991678299006205e-05, 'epoch': 0.16}\n",
      "{'loss': 1.1458, 'grad_norm': 44.05585479736328, 'learning_rate': 9.987570520365104e-05, 'epoch': 0.16}\n",
      "{'loss': 2.236, 'grad_norm': 102.86741638183594, 'learning_rate': 9.982642672195092e-05, 'epoch': 0.17}\n",
      "{'loss': 1.3732, 'grad_norm': 45.03480529785156, 'learning_rate': 9.976895564745991e-05, 'epoch': 0.18}\n",
      "{'loss': 1.1654, 'grad_norm': 41.626731872558594, 'learning_rate': 9.970330142972401e-05, 'epoch': 0.19}\n",
      "{'loss': 1.0776, 'grad_norm': 39.58439636230469, 'learning_rate': 9.962947486378326e-05, 'epoch': 0.19}\n",
      "{'loss': 1.5425, 'grad_norm': 71.67948913574219, 'learning_rate': 9.954748808839674e-05, 'epoch': 0.2}\n",
      "{'loss': 0.9318, 'grad_norm': 29.308544158935547, 'learning_rate': 9.945735458404681e-05, 'epoch': 0.21}\n",
      "{'loss': 0.8821, 'grad_norm': 28.700151443481445, 'learning_rate': 9.935908917072252e-05, 'epoch': 0.22}\n",
      "{'loss': 0.8274, 'grad_norm': 26.274015426635742, 'learning_rate': 9.925270800548285e-05, 'epoch': 0.22}\n",
      "{'loss': 1.3946, 'grad_norm': 46.80450439453125, 'learning_rate': 9.91382285798002e-05, 'epoch': 0.23}\n",
      "{'loss': 1.0569, 'grad_norm': 29.580591201782227, 'learning_rate': 9.901566971668437e-05, 'epoch': 0.24}\n",
      "{'loss': 1.8023, 'grad_norm': 94.31272888183594, 'learning_rate': 9.888505156758759e-05, 'epoch': 0.25}\n",
      "{'loss': 2.1889, 'grad_norm': 60.785400390625, 'learning_rate': 9.874639560909117e-05, 'epoch': 0.26}\n",
      "{'loss': 1.4408, 'grad_norm': 62.2938232421875, 'learning_rate': 9.859972463937441e-05, 'epoch': 0.26}\n",
      "{'loss': 1.4522, 'grad_norm': 64.03620910644531, 'learning_rate': 9.844506277446577e-05, 'epoch': 0.27}\n",
      "{'loss': 1.9216, 'grad_norm': 82.44741821289062, 'learning_rate': 9.828243544427796e-05, 'epoch': 0.28}\n",
      "{'loss': 1.0935, 'grad_norm': 29.414709091186523, 'learning_rate': 9.811186938842645e-05, 'epoch': 0.29}\n",
      "{'loss': 0.9619, 'grad_norm': 45.60808181762695, 'learning_rate': 9.793339265183303e-05, 'epoch': 0.29}\n",
      "{'loss': 0.9411, 'grad_norm': 36.12400436401367, 'learning_rate': 9.774703458011453e-05, 'epoch': 0.3}\n",
      "{'loss': 1.4645, 'grad_norm': 42.31729507446289, 'learning_rate': 9.755282581475769e-05, 'epoch': 0.31}\n",
      "{'loss': 1.4694, 'grad_norm': 23.46872329711914, 'learning_rate': 9.735079828808107e-05, 'epoch': 0.32}\n",
      "{'loss': 0.8438, 'grad_norm': 58.578758239746094, 'learning_rate': 9.714098521798465e-05, 'epoch': 0.33}\n",
      "{'loss': 0.7867, 'grad_norm': 24.493301391601562, 'learning_rate': 9.692342110248802e-05, 'epoch': 0.33}\n",
      "{'loss': 1.0246, 'grad_norm': 45.50068283081055, 'learning_rate': 9.669814171405816e-05, 'epoch': 0.34}\n",
      "{'loss': 0.982, 'grad_norm': 20.752655029296875, 'learning_rate': 9.64651840937276e-05, 'epoch': 0.35}\n",
      "{'loss': 1.0529, 'grad_norm': 64.11137390136719, 'learning_rate': 9.622458654500409e-05, 'epoch': 0.36}\n",
      "{'loss': 1.0596, 'grad_norm': 83.99830627441406, 'learning_rate': 9.597638862757255e-05, 'epoch': 0.36}\n",
      "{'loss': 0.9673, 'grad_norm': 42.31806182861328, 'learning_rate': 9.572063115079063e-05, 'epoch': 0.37}\n",
      "{'loss': 0.8083, 'grad_norm': 59.35894012451172, 'learning_rate': 9.545735616697875e-05, 'epoch': 0.38}\n",
      "{'loss': 0.894, 'grad_norm': 46.380149841308594, 'learning_rate': 9.518660696450568e-05, 'epoch': 0.39}\n",
      "{'loss': 1.7526, 'grad_norm': 49.14072036743164, 'learning_rate': 9.490842806067095e-05, 'epoch': 0.4}\n",
      "{'loss': 1.2764, 'grad_norm': 59.93259048461914, 'learning_rate': 9.46228651943853e-05, 'epoch': 0.4}\n",
      "{'loss': 1.5458, 'grad_norm': 33.58521270751953, 'learning_rate': 9.432996531865002e-05, 'epoch': 0.41}\n",
      "{'loss': 0.8544, 'grad_norm': 35.19160842895508, 'learning_rate': 9.40297765928369e-05, 'epoch': 0.42}\n",
      "{'loss': 0.6052, 'grad_norm': 35.09902572631836, 'learning_rate': 9.372234837476978e-05, 'epoch': 0.43}\n",
      "{'loss': 0.6411, 'grad_norm': 35.710933685302734, 'learning_rate': 9.340773121260893e-05, 'epoch': 0.43}\n",
      "{'loss': 1.3451, 'grad_norm': 68.46755981445312, 'learning_rate': 9.308597683653975e-05, 'epoch': 0.44}\n",
      "{'loss': 1.2903, 'grad_norm': 35.57858657836914, 'learning_rate': 9.275713815026731e-05, 'epoch': 0.45}\n",
      "{'loss': 1.2075, 'grad_norm': 55.88814163208008, 'learning_rate': 9.242126922231763e-05, 'epoch': 0.46}\n",
      "{'loss': 0.9481, 'grad_norm': 51.770992279052734, 'learning_rate': 9.207842527714767e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0039, 'grad_norm': 56.341705322265625, 'learning_rate': 9.172866268606513e-05, 'epoch': 0.47}\n",
      "{'loss': 0.6863, 'grad_norm': 14.780146598815918, 'learning_rate': 9.137203895795983e-05, 'epoch': 0.48}\n",
      "{'loss': 0.4984, 'grad_norm': 36.53889846801758, 'learning_rate': 9.10086127298478e-05, 'epoch': 0.49}\n",
      "{'loss': 0.501, 'grad_norm': 27.930465698242188, 'learning_rate': 9.063844375723014e-05, 'epoch': 0.5}\n",
      "{'loss': 1.99, 'grad_norm': 46.82979965209961, 'learning_rate': 9.02615929042678e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2577, 'grad_norm': 3.2652676105499268, 'learning_rate': 8.987812213377424e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0325, 'grad_norm': 25.34185218811035, 'learning_rate': 8.948809449702711e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2328, 'grad_norm': 91.02802276611328, 'learning_rate': 8.90915741234015e-05, 'epoch': 0.53}\n",
      "{'loss': 1.247, 'grad_norm': 60.97403335571289, 'learning_rate': 8.868862620982534e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5514, 'grad_norm': 55.11363220214844, 'learning_rate': 8.827931701005974e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2632, 'grad_norm': 31.90679359436035, 'learning_rate': 8.786371382380528e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5109, 'grad_norm': 34.462249755859375, 'learning_rate': 8.744188498563641e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2948, 'grad_norm': 24.299753189086914, 'learning_rate': 8.701389985376578e-05, 'epoch': 0.57}\n",
      "{'loss': 2.8427, 'grad_norm': 87.16333770751953, 'learning_rate': 8.657982879864007e-05, 'epoch': 0.57}\n",
      "{'loss': 1.7103, 'grad_norm': 94.39997100830078, 'learning_rate': 8.613974319136958e-05, 'epoch': 0.58}\n",
      "{'loss': 1.5944, 'grad_norm': 29.48086929321289, 'learning_rate': 8.569371539199316e-05, 'epoch': 0.59}\n",
      "{'loss': 0.8514, 'grad_norm': 31.131391525268555, 'learning_rate': 8.524181873758059e-05, 'epoch': 0.6}\n",
      "{'loss': 1.7986, 'grad_norm': 41.144229888916016, 'learning_rate': 8.478412753017433e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2786, 'grad_norm': 40.222686767578125, 'learning_rate': 8.432071702457252e-05, 'epoch': 0.61}\n",
      "{'loss': 1.637, 'grad_norm': 17.17293930053711, 'learning_rate': 8.385166341595548e-05, 'epoch': 0.62}\n",
      "{'loss': 1.2009, 'grad_norm': 24.621179580688477, 'learning_rate': 8.33770438273574e-05, 'epoch': 0.63}\n",
      "{'loss': 1.6302, 'grad_norm': 80.12905883789062, 'learning_rate': 8.289693629698564e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9575, 'grad_norm': 50.87936019897461, 'learning_rate': 8.241141976538943e-05, 'epoch': 0.64}\n",
      "{'loss': 1.5724, 'grad_norm': 34.93241882324219, 'learning_rate': 8.192057406248028e-05, 'epoch': 0.65}\n",
      "{'loss': 0.8778, 'grad_norm': 12.883296012878418, 'learning_rate': 8.142447989440618e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7716, 'grad_norm': 59.636077880859375, 'learning_rate': 8.092321883028158e-05, 'epoch': 0.67}\n",
      "{'loss': 1.4336, 'grad_norm': 35.4007453918457, 'learning_rate': 8.041687328877567e-05, 'epoch': 0.67}\n",
      "{'loss': 1.3151, 'grad_norm': 55.901084899902344, 'learning_rate': 7.990552652456081e-05, 'epoch': 0.68}\n",
      "{'loss': 1.1834, 'grad_norm': 79.02882385253906, 'learning_rate': 7.938926261462366e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7879, 'grad_norm': 48.243736267089844, 'learning_rate': 7.886816644444098e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5272, 'grad_norm': 12.882582664489746, 'learning_rate': 7.83423236940225e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3515, 'grad_norm': 26.20993423461914, 'learning_rate': 7.781182082382325e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8073, 'grad_norm': 37.339866638183594, 'learning_rate': 7.727674506052743e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7927, 'grad_norm': 41.745826721191406, 'learning_rate': 7.673718438270648e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0248, 'grad_norm': 51.81663513183594, 'learning_rate': 7.619322750635327e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtrainer_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: (compute_metrics(x, METRICS_DICT)),\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m training_outs \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.10/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.10/site-packages/transformers/trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2522\u001b[0m )\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2530\u001b[0m ):\n\u001b[1;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.10/site-packages/transformers/trainer.py:3687\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3685\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3687\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3688\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.10/site-packages/accelerate/accelerator.py:2244\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2244\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=trainer_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda x: (compute_metrics(x, METRICS_DICT)),\n",
    ")\n",
    "training_outs = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(os.path.join(trainer_config[\"logging_dir\"], \"model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import LlamaForSequenceClassification, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# ################################################################################\n",
    "# # bitsandbytes parameters\n",
    "# ################################################################################ \n",
    "# use_4bit = True # Activate 4-bit precision base model loading\n",
    "# bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models \n",
    "# bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
    "# use_nested_quant = False # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "# compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=use_4bit,\n",
    "#     bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "#     bnb_4bit_compute_dtype=compute_dtype,\n",
    "#     bnb_4bit_use_double_quant=use_nested_quant,\n",
    "# )\n",
    "# raw_model = LlamaForSequenceClassification.from_pretrained(\n",
    "#     \"meta-llama/Llama-2-7b-hf\",\n",
    "#     quantization_config=bnb_config,\n",
    "#     num_labels=3,\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     device_map=\"cuda:0\",\n",
    "# )\n",
    "# # Check GPU compatibility with bfloat16\n",
    "# if compute_dtype == torch.float16 and use_4bit:\n",
    "#     major, _ = torch.cuda.get_device_capability()\n",
    "#     if major >= 8:\n",
    "#         print(\"=\" * 80)\n",
    "#         print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "#         print(\"=\" * 80)\n",
    "# from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     raw_model,\n",
    "#     \"/home/harsh/Desktop/Projects/LLMs/SentimentClassification/TrainingLogs/runs/llama_test/model\",\n",
    "# )\n",
    "# model = model.merge_and_unload()\n",
    "# # lora_r = 16\n",
    "# # lora_alpha = 64\n",
    "# # lora_dropout = 0.1\n",
    "# # lora_target_modules = [\n",
    "# #     \"q_proj\",\n",
    "# #     \"up_proj\",\n",
    "# #     \"o_proj\",\n",
    "# #     \"k_proj\",\n",
    "# #     \"down_proj\",\n",
    "# #     \"gate_proj\",\n",
    "# #     \"v_proj\",\n",
    "# # ]\n",
    "\n",
    "\n",
    "# # peft_config = LoraConfig(\n",
    "# #     r=lora_r,\n",
    "# #     lora_alpha=lora_alpha,\n",
    "# #     lora_dropout=lora_dropout,\n",
    "# #     target_modules=lora_target_modules,\n",
    "# #     bias=\"none\",\n",
    "# #     task_type=\"SEQ_CLS\",\n",
    "# # )\n",
    "\n",
    "# # model = prepare_model_for_kbit_training(model)\n",
    "# # model = get_peft_model(model, peft_config)\n",
    "# # model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "prompt = \"Just listening to whitney houston, celin dione and mariah carey.... just a perfect sunday,, true talent there ladies and gents\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]),\n",
       " tensor([0], device='cuda:0'),\n",
       " {0: 'positive', 1: 'neutral', 2: 'negative'})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs.input_ids.to(device='cuda:0'))\n",
    "outputs.logits.shape, outputs.logits.argmax(dim=-1), train_dataset.index2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Just listening to whitney houston, celin dione and mariah carey.... just a perfect sunday,, true talent there ladies and gents ',\n",
       " 'source': 'sem_eval_2017',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_split_config,\n",
    "    get_dataset_config,\n",
    "    get_trainer_config,\n",
    "    get_prediction_config,\n",
    "    get_lora_config,\n",
    "    _handle_seed,\n",
    "    DEVICE,\n",
    ")\n",
    "from sentiment_dataset import SentimentDataset\n",
    "from model import (\n",
    "    load_model,\n",
    "    load_tokenizer,\n",
    "    load_lora_model,\n",
    "    predict,\n",
    "    compute_metrics,\n",
    "    METRICS_DICT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_config': {'desc': 'Multilinuagal sentiment classification dataset',\n",
       "  'test': {'dataset_path': 'tyqiangz/multilingual-sentiments',\n",
       "   'dataset_language': 'english',\n",
       "   'return_dict': True,\n",
       "   'padding': 'longest',\n",
       "   'max_length': 512,\n",
       "   'sub_split_size': None}},\n",
       " 'tokenizer_config': {'tokenizer_name': 'llama2',\n",
       "  'tokenizer_path': 'meta-llama/Llama-2-7b-hf'},\n",
       " 'model_config': {'model_name': 'llama2-finetuned',\n",
       "  'model_path': '/home/harsh/Desktop/Projects/LLMs/SentimentClassification/TrainingLogs/runs/llama2_english/model',\n",
       "  'base_model_path': 'meta-llama/Llama-2-7b-hf',\n",
       "  'num_labels': 3,\n",
       "  'force_download': False,\n",
       "  'device_map': 'cuda:0',\n",
       "  'bnb_config': {'load_in_4bit': True,\n",
       "   'bnb_4bit_quant_type': 'nf4',\n",
       "   'bnb_4bit_compute_dtype': 'float16',\n",
       "   'bnb_4bit_use_double_quant': False}},\n",
       " 'prediction_config': {'samples': ['what was the irony?',\n",
       "   'Gisburn was very curious']}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = read_yaml(\"./configs/predict_config_llama.yaml\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, model_path, base_model_path, model_config = get_model_config(config)\n",
    "model_config.update(dict(pad_token_id=tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019d189cd6f741658e49dd7921ad7a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\n",
    "    model_string=model_name,\n",
    "    model_path=model_path,\n",
    "    base_model_path=base_model_path,\n",
    "    model_config=model_config,\n",
    ")\n",
    "model = model.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_split_config,\n",
    "    get_dataset_config,\n",
    "    get_trainer_config,\n",
    "    get_prediction_config,\n",
    "    get_lora_config,\n",
    "    _handle_seed,\n",
    "    DEVICE,\n",
    ")\n",
    "from sentiment_dataset import SentimentDataset\n",
    "from model import (\n",
    "    load_model,\n",
    "    load_tokenizer,\n",
    "    load_lora_model,\n",
    "    predict,\n",
    "    compute_metrics,\n",
    "    METRICS_DICT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_yaml(\"./configs/train_config_gpt2.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_desc, (train_split_config, val_split_config, test_split_config) = (\n",
    "#     get_split_config(config)\n",
    "# )\n",
    "\n",
    "# test_dataset_path, test_dataset_language, test_dataset_config = (\n",
    "#     get_dataset_config(test_split_config)\n",
    "# )\n",
    "# test_dataset = SentimentDataset(\n",
    "#     dataset_path=test_dataset_path,\n",
    "#     dataset_language=test_dataset_language,\n",
    "#     split_type=\"test\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     **test_dataset_config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2',\n",
       " 'openai-community/gpt2',\n",
       " None,\n",
       " {'num_labels': 3, 'pad_token_id': 50256})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name, model_path, base_model_path, model_config = get_model_config(config)\n",
    "model_config.update(dict(pad_token_id=tokenizer.pad_token_id))\n",
    "model_name, model_path, base_model_path, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\n",
    "    model_string=model_name,\n",
    "    model_path=model_path,\n",
    "    base_model_path=base_model_path,\n",
    "    model_config=model_config,\n",
    ")\n",
    "model = model.to(device=DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4858a1d0d364591ae2b4bd8a03d3ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/37.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fa09d34db14806864b57e172edc6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/1.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae61d5f04914d76812e0c0c87bc4af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/1.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388daf9925594d6da8c7d6ac2e61d080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/270399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e9587217f543cd93661354fa96f6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6340ec34416c42588c0b9d76c976529a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/14465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", 'all', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_dataset.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yang memerlukan pemerhatian dan tindakan serius</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentiasa memikirkan dan merancang inisiatif ba...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kita akan tengok daripada pelbagai aspek supay...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>justeru asean perlu mengambil tindakan sebagai...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@_Niiar_ Jangan punah dulu, aku belum ke labua...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270394</th>\n",
       "      <td>RT @user: #Ø£ÙŠÙÙˆÙ†_Ø§Ù„Ø¨Ø±ÙˆÙÙŠØ³ÙˆØ±Ù…Ø³Ø§Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ø£ÙŠÙÙˆÙ†7 Ø£...</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270395</th>\n",
       "      <td>Ø§Ù„Ù„Ù‡Ù… Ø£Ù†Øª Ø§Ù„Ø³Ù„Ø§Ù… ÙˆÙ…Ù†Ùƒ Ø§Ù„Ø³Ù„Ø§Ù… ØªØ¨Ø§Ø±ÙƒØª ÙŠØ§ Ø°Ø§ Ø§Ù„Ø¬Ù„...</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270396</th>\n",
       "      <td>Ø¹Ù„Ù‰ ÙˆÙ‚Ø¹ Ø­Ù…Ù‰ Ø§Ù„Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Ø§Ù„Ø£Ù…ÙŠØ±ÙƒÙŠØ© ÙˆÙŠÙƒÙ„ÙŠÙƒØ³ ØªÙƒØ´Ù ...</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270397</th>\n",
       "      <td>@user @user Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠ Ø³ÙŠ\"ÙˆÙŠÙ†Ø¯ÙˆØ² 10\"</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270398</th>\n",
       "      <td>Ø¹Ù†Ø¯ÙŠ ÙˆØ­Ø¯Ù‡ ØªØ´Ø¨Ù‡ Ù‡Ø§Ø±ÙŠ Ø¨ÙˆØªØ± ÙŠØ§ Ø§Ù†Ù‡Ø§ ÙƒÙŠÙˆØªÙŠÙ‡ğŸ˜¨!ğŸ˜­ğŸ’—</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270399 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text         source  \\\n",
       "0         yang memerlukan pemerhatian dan tindakan serius         malaya   \n",
       "1       sentiasa memikirkan dan merancang inisiatif ba...         malaya   \n",
       "2       Kita akan tengok daripada pelbagai aspek supay...         malaya   \n",
       "3       justeru asean perlu mengambil tindakan sebagai...         malaya   \n",
       "4       @_Niiar_ Jangan punah dulu, aku belum ke labua...         malaya   \n",
       "...                                                   ...            ...   \n",
       "270394  RT @user: #Ø£ÙŠÙÙˆÙ†_Ø§Ù„Ø¨Ø±ÙˆÙÙŠØ³ÙˆØ±Ù…Ø³Ø§Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ø£ÙŠÙÙˆÙ†7 Ø£...  sem_eval_2017   \n",
       "270395  Ø§Ù„Ù„Ù‡Ù… Ø£Ù†Øª Ø§Ù„Ø³Ù„Ø§Ù… ÙˆÙ…Ù†Ùƒ Ø§Ù„Ø³Ù„Ø§Ù… ØªØ¨Ø§Ø±ÙƒØª ÙŠØ§ Ø°Ø§ Ø§Ù„Ø¬Ù„...  sem_eval_2017   \n",
       "270396  Ø¹Ù„Ù‰ ÙˆÙ‚Ø¹ Ø­Ù…Ù‰ Ø§Ù„Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Ø§Ù„Ø£Ù…ÙŠØ±ÙƒÙŠØ© ÙˆÙŠÙƒÙ„ÙŠÙƒØ³ ØªÙƒØ´Ù ...  sem_eval_2017   \n",
       "270397                 @user @user Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠ Ø³ÙŠ\"ÙˆÙŠÙ†Ø¯ÙˆØ² 10\"  sem_eval_2017   \n",
       "270398        Ø¹Ù†Ø¯ÙŠ ÙˆØ­Ø¯Ù‡ ØªØ´Ø¨Ù‡ Ù‡Ø§Ø±ÙŠ Ø¨ÙˆØªØ± ÙŠØ§ Ø§Ù†Ù‡Ø§ ÙƒÙŠÙˆØªÙŠÙ‡ğŸ˜¨!ğŸ˜­ğŸ’—  sem_eval_2017   \n",
       "\n",
       "       language  label  \n",
       "0         malay      0  \n",
       "1         malay      0  \n",
       "2         malay      0  \n",
       "3         malay      0  \n",
       "4         malay      1  \n",
       "...         ...    ...  \n",
       "270394   arabic      1  \n",
       "270395   arabic      0  \n",
       "270396   arabic      2  \n",
       "270397   arabic      1  \n",
       "270398   arabic      0  \n",
       "\n",
       "[270399 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(train_dataset.to_dict())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import load_pretrained_gpt2_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_pretrained_gpt2_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [51, 316, 15042, 275, 10102, 393, 648, 300, 391, 1066, 263, 600, 993, 479, 5350, 479, 1045, 256, 461, 731, 377, 11, 256, 461, 1888, 283, 11, 256, 461, 16486, 84, 11, 256, 461, 1458, 315], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(df['text'][2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ø¹Ù†Ø¯ÙŠ ÙˆØ­Ø¯Ù‡ ØªØ´Ø¨Ù‡ Ù‡Ø§Ø±ÙŠ Ø¨ÙˆØªØ± ÙŠØ§ Ø§Ù†Ù‡Ø§ ÙƒÙŠÙˆØªÙŠÙ‡ğŸ˜¨!ğŸ˜­ğŸ’—'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][270398]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_tokenizer_config,\n",
    "    get_dataset_config,\n",
    "    DEVICE,\n",
    ")\n",
    "from gpt_dataset import GPTDataset\n",
    "from model import load_model, load_tokenizer, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "config = read_yaml('./train_config.yaml')\n",
    "\n",
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(tokenizer_name, tokenizer_path)\n",
    "\n",
    "dataset_path, dataset_desc, dataset_config = get_dataset_config(config)\n",
    "train_dataset = GPTDataset(\n",
    "    filepath=dataset_path, tokenizer=tokenizer, **dataset_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD alwa'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./dataset/the-verdict.txt', 'r') as F:\n",
    "    file_text = F.read()\n",
    "file_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_split_config,\n",
    "    get_dataset_config,\n",
    "    get_trainer_config,\n",
    "    get_prediction_config,\n",
    "    get_lora_config,\n",
    "    _handle_seed,\n",
    "    DEVICE,\n",
    ")\n",
    "from sentiment_dataset import SentimentDataset\n",
    "from model import (\n",
    "    load_model,\n",
    "    load_tokenizer,\n",
    "    load_lora_model,\n",
    "    predict,\n",
    "    compute_metrics,\n",
    "    METRICS_DICT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_yaml(\"./configs/train_config_llama.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_desc, (train_split_config, val_split_config, test_split_config) = (\n",
    "    get_split_config(config)\n",
    ")\n",
    "\n",
    "test_dataset_path, test_dataset_language, test_dataset_config = (\n",
    "    get_dataset_config(test_split_config)\n",
    ")\n",
    "test_dataset = SentimentDataset(\n",
    "    dataset_path=test_dataset_path,\n",
    "    dataset_language=test_dataset_language,\n",
    "    split_type=\"test\",\n",
    "    tokenizer=tokenizer,\n",
    "    **test_dataset_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4b48e9c4a44420a16f0f0c43f343cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name, model_path, base_model_path, model_config = get_model_config(config)\n",
    "model_config.update(dict(pad_token_id=tokenizer.pad_token_id))\n",
    "model = load_model(\n",
    "    model_string=model_name,\n",
    "    model_path=model_path,\n",
    "    base_model_path=base_model_path,\n",
    "    model_config=model_config,\n",
    ")\n",
    "model = model.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_samples = [\n",
    "    \"this is a bad product\",\n",
    "    \"this the nice product\",\n",
    "    \"this is a best product\",\n",
    "    \"how are you?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 0, 1]), ['negative', 'positive', 'positive', 'neutral'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predcitions = predict(\n",
    "    model, tokenizer, prompt_samples, device=DEVICE,\n",
    ")\n",
    "predcitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=test_dataset_config.get(\"padding\", True),\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda x: (compute_metrics(x, {'accuracy': evaluate.load(\"accuracy\")})),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1453b145fdf5481fbe7727b0517cd6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.099609375,\n",
       " 'eval_model_preparation_time': 0.0041,\n",
       " 'eval_accuracy': 0.73103,\n",
       " 'eval_runtime': 46.9713,\n",
       " 'eval_samples_per_second': 18.522,\n",
       " 'eval_steps_per_second': 2.321}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

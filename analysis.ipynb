{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", 'all', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_dataset.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yang memerlukan pemerhatian dan tindakan serius</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentiasa memikirkan dan merancang inisiatif ba...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kita akan tengok daripada pelbagai aspek supay...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>justeru asean perlu mengambil tindakan sebagai...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@_Niiar_ Jangan punah dulu, aku belum ke labua...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270394</th>\n",
       "      <td>RT @user: #Ø£ÙŠÙÙˆÙ†_Ø§Ù„Ø¨Ø±ÙˆÙÙŠØ³ÙˆØ±Ù…Ø³Ø§Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ø£ÙŠÙÙˆÙ†7 Ø£...</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270395</th>\n",
       "      <td>Ø§Ù„Ù„Ù‡Ù… Ø£Ù†Øª Ø§Ù„Ø³Ù„Ø§Ù… ÙˆÙ…Ù†Ùƒ Ø§Ù„Ø³Ù„Ø§Ù… ØªØ¨Ø§Ø±ÙƒØª ÙŠØ§ Ø°Ø§ Ø§Ù„Ø¬Ù„...</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270396</th>\n",
       "      <td>Ø¹Ù„Ù‰ ÙˆÙ‚Ø¹ Ø­Ù…Ù‰ Ø§Ù„Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Ø§Ù„Ø£Ù…ÙŠØ±ÙƒÙŠØ© ÙˆÙŠÙƒÙ„ÙŠÙƒØ³ ØªÙƒØ´Ù ...</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270397</th>\n",
       "      <td>@user @user Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠ Ø³ÙŠ\"ÙˆÙŠÙ†Ø¯ÙˆØ² 10\"</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270398</th>\n",
       "      <td>Ø¹Ù†Ø¯ÙŠ ÙˆØ­Ø¯Ù‡ ØªØ´Ø¨Ù‡ Ù‡Ø§Ø±ÙŠ Ø¨ÙˆØªØ± ÙŠØ§ Ø§Ù†Ù‡Ø§ ÙƒÙŠÙˆØªÙŠÙ‡ğŸ˜¨!ğŸ˜­ğŸ’—</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270399 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text         source  \\\n",
       "0         yang memerlukan pemerhatian dan tindakan serius         malaya   \n",
       "1       sentiasa memikirkan dan merancang inisiatif ba...         malaya   \n",
       "2       Kita akan tengok daripada pelbagai aspek supay...         malaya   \n",
       "3       justeru asean perlu mengambil tindakan sebagai...         malaya   \n",
       "4       @_Niiar_ Jangan punah dulu, aku belum ke labua...         malaya   \n",
       "...                                                   ...            ...   \n",
       "270394  RT @user: #Ø£ÙŠÙÙˆÙ†_Ø§Ù„Ø¨Ø±ÙˆÙÙŠØ³ÙˆØ±Ù…Ø³Ø§Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ø£ÙŠÙÙˆÙ†7 Ø£...  sem_eval_2017   \n",
       "270395  Ø§Ù„Ù„Ù‡Ù… Ø£Ù†Øª Ø§Ù„Ø³Ù„Ø§Ù… ÙˆÙ…Ù†Ùƒ Ø§Ù„Ø³Ù„Ø§Ù… ØªØ¨Ø§Ø±ÙƒØª ÙŠØ§ Ø°Ø§ Ø§Ù„Ø¬Ù„...  sem_eval_2017   \n",
       "270396  Ø¹Ù„Ù‰ ÙˆÙ‚Ø¹ Ø­Ù…Ù‰ Ø§Ù„Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Ø§Ù„Ø£Ù…ÙŠØ±ÙƒÙŠØ© ÙˆÙŠÙƒÙ„ÙŠÙƒØ³ ØªÙƒØ´Ù ...  sem_eval_2017   \n",
       "270397                 @user @user Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠ Ø³ÙŠ\"ÙˆÙŠÙ†Ø¯ÙˆØ² 10\"  sem_eval_2017   \n",
       "270398        Ø¹Ù†Ø¯ÙŠ ÙˆØ­Ø¯Ù‡ ØªØ´Ø¨Ù‡ Ù‡Ø§Ø±ÙŠ Ø¨ÙˆØªØ± ÙŠØ§ Ø§Ù†Ù‡Ø§ ÙƒÙŠÙˆØªÙŠÙ‡ğŸ˜¨!ğŸ˜­ğŸ’—  sem_eval_2017   \n",
       "\n",
       "       language  label  \n",
       "0         malay      0  \n",
       "1         malay      0  \n",
       "2         malay      0  \n",
       "3         malay      0  \n",
       "4         malay      1  \n",
       "...         ...    ...  \n",
       "270394   arabic      1  \n",
       "270395   arabic      0  \n",
       "270396   arabic      2  \n",
       "270397   arabic      1  \n",
       "270398   arabic      0  \n",
       "\n",
       "[270399 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(train_dataset.to_dict())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import load_pretrained_gpt2_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_pretrained_gpt2_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [51, 316, 15042, 275, 10102, 393, 648, 300, 391, 1066, 263, 600, 993, 479, 5350, 479, 1045, 256, 461, 731, 377, 11, 256, 461, 1888, 283, 11, 256, 461, 16486, 84, 11, 256, 461, 1458, 315], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(df['text'][2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ø¹Ù†Ø¯ÙŠ ÙˆØ­Ø¯Ù‡ ØªØ´Ø¨Ù‡ Ù‡Ø§Ø±ÙŠ Ø¨ÙˆØªØ± ÙŠØ§ Ø§Ù†Ù‡Ø§ ÙƒÙŠÙˆØªÙŠÙ‡ğŸ˜¨!ğŸ˜­ğŸ’—'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][270398]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_tokenizer_config,\n",
    "    get_dataset_config,\n",
    "    DEVICE,\n",
    ")\n",
    "from gpt_dataset import GPTDataset\n",
    "from model import load_model, load_tokenizer, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "config = read_yaml('./train_config.yaml')\n",
    "\n",
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(tokenizer_name, tokenizer_path)\n",
    "\n",
    "dataset_path, dataset_desc, dataset_config = get_dataset_config(config)\n",
    "train_dataset = GPTDataset(\n",
    "    filepath=dataset_path, tokenizer=tokenizer, **dataset_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD alwa'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./dataset/the-verdict.txt', 'r') as F:\n",
    "    file_text = F.read()\n",
    "file_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_split_config,\n",
    "    get_dataset_config,\n",
    "    get_trainer_config,\n",
    "    get_prediction_config,\n",
    "    get_lora_config,\n",
    "    _handle_seed,\n",
    "    DEVICE,\n",
    ")\n",
    "from sentiment_dataset import SentimentDataset\n",
    "from model import (\n",
    "    load_model,\n",
    "    load_tokenizer,\n",
    "    load_lora_model,\n",
    "    predict,\n",
    "    compute_metrics,\n",
    "    METRICS_DICT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_yaml(\"./configs/predict_config_llama_lora.yaml\")\n",
    "# config = read_yaml(\"./configs/predict_config_gpt2.yaml\")\n",
    "# config = read_yaml(\"./configs/predict_config_llama.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_desc, (train_split_config, val_split_config, test_split_config) = (\n",
    "    get_split_config(config)\n",
    ")\n",
    "\n",
    "test_dataset_path, test_dataset_language, test_dataset_config = (\n",
    "    get_dataset_config(test_split_config)\n",
    ")\n",
    "test_dataset = SentimentDataset(\n",
    "    dataset_path=test_dataset_path,\n",
    "    dataset_language=test_dataset_language,\n",
    "    split_type=\"test\",\n",
    "    tokenizer=tokenizer,\n",
    "    **test_dataset_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdce31eb34f467ead754aa35ab41b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name, model_path, base_model_path, model_config = get_model_config(config)\n",
    "model_config.update(dict(pad_token_id=tokenizer.pad_token_id))\n",
    "model = load_model(\n",
    "    model_string=model_name,\n",
    "    model_path=model_path,\n",
    "    base_model_path=base_model_path,\n",
    "    model_config=model_config,\n",
    ")\n",
    "model = model.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_samples = [\n",
    "    \"this is a bad product\",\n",
    "    \"this the nice product\",\n",
    "    \"this is a best product\",\n",
    "    \"how are you?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmankodi/bitsandbytes/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 0, 1]), ['negative', 'positive', 'positive', 'neutral'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predcitions = predict(\n",
    "    model, tokenizer, prompt_samples, device=DEVICE,\n",
    ")\n",
    "predcitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from model import METRICS_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=test_dataset_config.get(\"padding\", True),\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda x: (compute_metrics(x, METRICS_DICT)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1809' max='1809' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1809/1809 11:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0556640625,\n",
       " 'eval_model_preparation_time': 0.0045,\n",
       " 'eval_accuracy': 0.68766,\n",
       " 'eval_auc': 0.85569,\n",
       " 'eval_runtime': 690.2538,\n",
       " 'eval_samples_per_second': 20.956,\n",
       " 'eval_steps_per_second': 2.621}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lora_params(model):\n",
    "    \"\"\"\n",
    "    Prints and counts the total trainable parameters for LoRA layers in a model.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    print(\"LoRA Trainable Parameters:\\n\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_params = param.numel()\n",
    "            total_params += num_params\n",
    "            print(f\"{name}: {num_params} parameters\")\n",
    "    \n",
    "    print(f\"\\nTotal LoRA trainable parameters: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3409342464"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(i.numel() for i in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,288 || all params: 6,647,345,152 || trainable%: 0.0002\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

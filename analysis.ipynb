{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4858a1d0d364591ae2b4bd8a03d3ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/37.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fa09d34db14806864b57e172edc6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/1.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae61d5f04914d76812e0c0c87bc4af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/1.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388daf9925594d6da8c7d6ac2e61d080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/270399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e9587217f543cd93661354fa96f6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6340ec34416c42588c0b9d76c976529a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/14465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", 'all', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_dataset.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yang memerlukan pemerhatian dan tindakan serius</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentiasa memikirkan dan merancang inisiatif ba...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kita akan tengok daripada pelbagai aspek supay...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>justeru asean perlu mengambil tindakan sebagai...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@_Niiar_ Jangan punah dulu, aku belum ke labua...</td>\n",
       "      <td>malaya</td>\n",
       "      <td>malay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270394</th>\n",
       "      <td>RT @user: #أيفون_البروفيسورمسابقة على أيفون7 أ...</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270395</th>\n",
       "      <td>اللهم أنت السلام ومنك السلام تباركت يا ذا الجل...</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270396</th>\n",
       "      <td>على وقع حمى الانتخابات الأميركية ويكليكس تكشف ...</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270397</th>\n",
       "      <td>@user @user على البي سي\"ويندوز 10\"</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270398</th>\n",
       "      <td>عندي وحده تشبه هاري بوتر يا انها كيوتيه😨!😭💗</td>\n",
       "      <td>sem_eval_2017</td>\n",
       "      <td>arabic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270399 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text         source  \\\n",
       "0         yang memerlukan pemerhatian dan tindakan serius         malaya   \n",
       "1       sentiasa memikirkan dan merancang inisiatif ba...         malaya   \n",
       "2       Kita akan tengok daripada pelbagai aspek supay...         malaya   \n",
       "3       justeru asean perlu mengambil tindakan sebagai...         malaya   \n",
       "4       @_Niiar_ Jangan punah dulu, aku belum ke labua...         malaya   \n",
       "...                                                   ...            ...   \n",
       "270394  RT @user: #أيفون_البروفيسورمسابقة على أيفون7 أ...  sem_eval_2017   \n",
       "270395  اللهم أنت السلام ومنك السلام تباركت يا ذا الجل...  sem_eval_2017   \n",
       "270396  على وقع حمى الانتخابات الأميركية ويكليكس تكشف ...  sem_eval_2017   \n",
       "270397                 @user @user على البي سي\"ويندوز 10\"  sem_eval_2017   \n",
       "270398        عندي وحده تشبه هاري بوتر يا انها كيوتيه😨!😭💗  sem_eval_2017   \n",
       "\n",
       "       language  label  \n",
       "0         malay      0  \n",
       "1         malay      0  \n",
       "2         malay      0  \n",
       "3         malay      0  \n",
       "4         malay      1  \n",
       "...         ...    ...  \n",
       "270394   arabic      1  \n",
       "270395   arabic      0  \n",
       "270396   arabic      2  \n",
       "270397   arabic      1  \n",
       "270398   arabic      0  \n",
       "\n",
       "[270399 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(train_dataset.to_dict())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import load_pretrained_gpt2_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_pretrained_gpt2_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [51, 316, 15042, 275, 10102, 393, 648, 300, 391, 1066, 263, 600, 993, 479, 5350, 479, 1045, 256, 461, 731, 377, 11, 256, 461, 1888, 283, 11, 256, 461, 16486, 84, 11, 256, 461, 1458, 315], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(df['text'][2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'عندي وحده تشبه هاري بوتر يا انها كيوتيه😨!😭💗'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][270398]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_tokenizer_config,\n",
    "    get_dataset_config,\n",
    "    DEVICE,\n",
    ")\n",
    "from gpt_dataset import GPTDataset\n",
    "from model import load_model, load_tokenizer, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "config = read_yaml('./train_config.yaml')\n",
    "\n",
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(tokenizer_name, tokenizer_path)\n",
    "\n",
    "dataset_path, dataset_desc, dataset_config = get_dataset_config(config)\n",
    "train_dataset = GPTDataset(\n",
    "    filepath=dataset_path, tokenizer=tokenizer, **dataset_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD alwa'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./dataset/the-verdict.txt', 'r') as F:\n",
    "    file_text = F.read()\n",
    "file_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_split_config,\n",
    "    get_dataset_config,\n",
    "    get_trainer_config,\n",
    "    get_prediction_config,\n",
    "    get_lora_config,\n",
    "    _handle_seed,\n",
    "    DEVICE,\n",
    ")\n",
    "from sentiment_dataset import SentimentDataset\n",
    "from model import (\n",
    "    load_model,\n",
    "    load_tokenizer,\n",
    "    load_lora_model,\n",
    "    predict,\n",
    "    compute_metrics,\n",
    "    METRICS_DICT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_yaml(\"./configs/train_config_llama.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_desc, (train_split_config, val_split_config, test_split_config) = (\n",
    "    get_split_config(config)\n",
    ")\n",
    "\n",
    "test_dataset_path, test_dataset_language, test_dataset_config = (\n",
    "    get_dataset_config(test_split_config)\n",
    ")\n",
    "test_dataset = SentimentDataset(\n",
    "    dataset_path=test_dataset_path,\n",
    "    dataset_language=test_dataset_language,\n",
    "    split_type=\"test\",\n",
    "    tokenizer=tokenizer,\n",
    "    **test_dataset_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4b48e9c4a44420a16f0f0c43f343cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name, model_path, base_model_path, model_config = get_model_config(config)\n",
    "model_config.update(dict(pad_token_id=tokenizer.pad_token_id))\n",
    "model = load_model(\n",
    "    model_string=model_name,\n",
    "    model_path=model_path,\n",
    "    base_model_path=base_model_path,\n",
    "    model_config=model_config,\n",
    ")\n",
    "model = model.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_samples = [\n",
    "    \"this is a bad product\",\n",
    "    \"this the nice product\",\n",
    "    \"this is a best product\",\n",
    "    \"how are you?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 0, 1]), ['negative', 'positive', 'positive', 'neutral'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predcitions = predict(\n",
    "    model, tokenizer, prompt_samples, device=DEVICE,\n",
    ")\n",
    "predcitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=test_dataset_config.get(\"padding\", True),\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda x: (compute_metrics(x, {'accuracy': evaluate.load(\"accuracy\")})),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1453b145fdf5481fbe7727b0517cd6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.099609375,\n",
       " 'eval_model_preparation_time': 0.0041,\n",
       " 'eval_accuracy': 0.73103,\n",
       " 'eval_runtime': 46.9713,\n",
       " 'eval_samples_per_second': 18.522,\n",
       " 'eval_steps_per_second': 2.321}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
